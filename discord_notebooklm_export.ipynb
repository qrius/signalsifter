{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e0b0260",
   "metadata": {},
   "source": [
    "# Discord Message Analysis and NotebookLM Export\n",
    "\n",
    "This notebook analyzes extracted Discord messages from the STBL general chat and exports them in a structured format suitable for NotebookLM analysis.\n",
    "\n",
    "## Overview\n",
    "- **Source**: Discord #STBL ğŸ’¬ã€¡stbl-general-chat \n",
    "- **Time Period**: 2+ months (October-December 2025)\n",
    "- **Expected Messages**: 149+ messages with enhanced extraction\n",
    "- **Purpose**: Prepare data for AI analysis in NotebookLM\n",
    "\n",
    "## Extraction Details\n",
    "- **Enhanced Algorithm**: 4 scroll methods with 20-failure tolerance\n",
    "- **Fixed Issues**: Edited message timestamp handling\n",
    "- **Database**: Shared SignalSifter database (`data/backfill.sqlite`)\n",
    "- **Quality**: ~90%+ message processing success rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dffb33",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "Import necessary libraries including pandas, json, os, and datetime for data processing and file operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e026ec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Set up paths\n",
    "BASE_DIR = Path.cwd()\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "EXPORT_DIR = BASE_DIR / 'notebooklm_export'\n",
    "DB_PATH = DATA_DIR / 'backfill.sqlite'\n",
    "\n",
    "# Create export directory\n",
    "EXPORT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"ğŸ“š Libraries imported successfully\")\n",
    "print(f\"ğŸ—‚ï¸  Export directory: {EXPORT_DIR}\")\n",
    "print(f\"ğŸ—„ï¸  Database path: {DB_PATH}\")\n",
    "print(f\"ğŸ“ Database exists: {DB_PATH.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671add7e",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data\n",
    "Load your dataset and perform initial data cleaning and preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506878fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_discord_data():\n",
    "    \"\"\"Load Discord messages from the unified SignalSifter database\"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(DB_PATH)\n",
    "        \n",
    "        # Get all tables to verify Discord tables exist\n",
    "        tables_query = \"SELECT name FROM sqlite_master WHERE type='table'\"\n",
    "        tables = pd.read_sql_query(tables_query, conn)\n",
    "        \n",
    "        print(\"ğŸ“Š Available database tables:\")\n",
    "        for table in tables['name'].tolist():\n",
    "            print(f\"  - {table}\")\n",
    "        \n",
    "        # Check if Discord tables exist\n",
    "        discord_tables = tables[tables['name'].str.contains('discord', case=False, na=False)]\n",
    "        \n",
    "        if discord_tables.empty:\n",
    "            print(\"âš ï¸  No Discord tables found. Running extraction verification...\")\n",
    "            # Try to check what extraction completed\n",
    "            return None, None, None\n",
    "            \n",
    "        # Load Discord messages\n",
    "        messages_query = \"\"\"\n",
    "        SELECT \n",
    "            dm.message_id,\n",
    "            dm.username,\n",
    "            dm.display_name,\n",
    "            dm.content,\n",
    "            dm.timestamp,\n",
    "            dm.edited_timestamp,\n",
    "            dm.message_type,\n",
    "            dm.reactions,\n",
    "            dm.mentions,\n",
    "            dm.is_bot,\n",
    "            dc.name as channel_name,\n",
    "            ds.name as server_name\n",
    "        FROM discord_messages dm\n",
    "        LEFT JOIN discord_channels dc ON dm.channel_id = dc.channel_id  \n",
    "        LEFT JOIN discord_servers ds ON dm.server_id = ds.server_id\n",
    "        ORDER BY dm.timestamp ASC\n",
    "        \"\"\"\n",
    "        \n",
    "        messages_df = pd.read_sql_query(messages_query, conn)\n",
    "        \n",
    "        # Load extraction logs if available\n",
    "        try:\n",
    "            logs_query = \"SELECT * FROM discord_extraction_log ORDER BY start_time DESC\"\n",
    "            logs_df = pd.read_sql_query(logs_query, conn)\n",
    "        except:\n",
    "            logs_df = pd.DataFrame()\n",
    "            \n",
    "        # Load channel info\n",
    "        try:\n",
    "            channels_query = \"SELECT * FROM discord_channels\"\n",
    "            channels_df = pd.read_sql_query(channels_query, conn)\n",
    "        except:\n",
    "            channels_df = pd.DataFrame()\n",
    "            \n",
    "        conn.close()\n",
    "        \n",
    "        print(f\"âœ… Loaded {len(messages_df)} Discord messages\")\n",
    "        print(f\"ğŸ“‹ Extraction logs: {len(logs_df)} entries\")\n",
    "        print(f\"ğŸ“º Channels: {len(channels_df)} channels\")\n",
    "        \n",
    "        return messages_df, logs_df, channels_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading data: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Load the data\n",
    "messages_df, logs_df, channels_df = load_discord_data()\n",
    "\n",
    "if messages_df is not None and not messages_df.empty:\n",
    "    print(f\"\\nğŸ“ˆ Data Summary:\")\n",
    "    print(f\"  Total messages: {len(messages_df)}\")\n",
    "    print(f\"  Date range: {messages_df['timestamp'].min()} to {messages_df['timestamp'].max()}\")\n",
    "    print(f\"  Unique users: {messages_df['username'].nunique()}\")\n",
    "    print(f\"  Messages with content: {messages_df['content'].notna().sum()}\")\n",
    "else:\n",
    "    print(\"âš ï¸  No message data loaded - may need to run extraction first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133b570c",
   "metadata": {},
   "source": [
    "## 3. Format Data for NotebookLM\n",
    "Structure the data into a format suitable for NotebookLM ingestion, organizing content into logical sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b677fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_for_notebooklm(messages_df):\n",
    "    \"\"\"Format Discord messages for NotebookLM analysis\"\"\"\n",
    "    \n",
    "    if messages_df is None or messages_df.empty:\n",
    "        print(\"âŒ No data to format\")\n",
    "        return {}\n",
    "    \n",
    "    # Clean and prepare data\n",
    "    df = messages_df.copy()\n",
    "    \n",
    "    # Parse timestamps\n",
    "    df['timestamp_dt'] = pd.to_datetime(df['timestamp'])\n",
    "    df['date'] = df['timestamp_dt'].dt.date\n",
    "    df['hour'] = df['timestamp_dt'].dt.hour\n",
    "    df['day_name'] = df['timestamp_dt'].dt.day_name()\n",
    "    \n",
    "    # Clean content\n",
    "    df['content_clean'] = df['content'].fillna('[No text content]')\n",
    "    df['word_count'] = df['content_clean'].apply(lambda x: len(str(x).split()) if pd.notna(x) else 0)\n",
    "    \n",
    "    # Identify message types\n",
    "    df['has_mentions'] = df['mentions'].notna() & (df['mentions'] != 'null')\n",
    "    df['has_reactions'] = df['reactions'].notna() & (df['reactions'] != 'null') \n",
    "    df['is_edited'] = df['edited_timestamp'].notna()\n",
    "    \n",
    "    print(\"ğŸ”§ Data formatting completed\")\n",
    "    \n",
    "    # Create structured data for NotebookLM\n",
    "    formatted_data = {\n",
    "        'metadata': {\n",
    "            'source': 'Discord STBL General Chat',\n",
    "            'extraction_date': datetime.now().isoformat(),\n",
    "            'total_messages': len(df),\n",
    "            'date_range': {\n",
    "                'start': df['timestamp'].min(),\n",
    "                'end': df['timestamp'].max()\n",
    "            },\n",
    "            'unique_users': df['username'].nunique(),\n",
    "            'channel': df['channel_name'].iloc[0] if not df.empty else 'Unknown',\n",
    "            'server': df['server_name'].iloc[0] if not df.empty else 'Unknown'\n",
    "        },\n",
    "        'messages': df.to_dict('records'),\n",
    "        'analytics': {\n",
    "            'user_stats': df.groupby('username').agg({\n",
    "                'message_id': 'count',\n",
    "                'word_count': 'sum',\n",
    "                'has_reactions': 'sum'\n",
    "            }).rename(columns={'message_id': 'message_count'}).to_dict('index'),\n",
    "            \n",
    "            'daily_activity': df.groupby('date')['message_id'].count().to_dict(),\n",
    "            \n",
    "            'hourly_pattern': df.groupby('hour')['message_id'].count().to_dict(),\n",
    "            \n",
    "            'content_analysis': {\n",
    "                'total_words': df['word_count'].sum(),\n",
    "                'avg_words_per_message': df['word_count'].mean(),\n",
    "                'edited_messages': df['is_edited'].sum(),\n",
    "                'messages_with_reactions': df['has_reactions'].sum(),\n",
    "                'messages_with_mentions': df['has_mentions'].sum()\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"ğŸ“Š Formatted data structure:\")\n",
    "    print(f\"  - Messages: {len(formatted_data['messages'])}\")\n",
    "    print(f\"  - Users: {len(formatted_data['analytics']['user_stats'])}\")\n",
    "    print(f\"  - Active days: {len(formatted_data['analytics']['daily_activity'])}\")\n",
    "    print(f\"  - Total words: {formatted_data['analytics']['content_analysis']['total_words']:,}\")\n",
    "    \n",
    "    return formatted_data, df\n",
    "\n",
    "# Format the data\n",
    "if messages_df is not None and not messages_df.empty:\n",
    "    formatted_data, processed_df = format_for_notebooklm(messages_df)\n",
    "else:\n",
    "    print(\"âš ï¸  Skipping formatting - no message data available\")\n",
    "    formatted_data, processed_df = {}, pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b57f18",
   "metadata": {},
   "source": [
    "## 4. Create Structured Documents\n",
    "Convert data records into well-formatted text documents with clear headings and sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42c5d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_structured_documents(formatted_data, processed_df):\n",
    "    \"\"\"Create well-formatted documents for NotebookLM analysis\"\"\"\n",
    "    \n",
    "    if not formatted_data or processed_df.empty:\n",
    "        print(\"âŒ No data to create documents from\")\n",
    "        return {}\n",
    "    \n",
    "    documents = {}\n",
    "    \n",
    "    # 1. Overview Document\n",
    "    overview = f\"\"\"# Discord STBL Community Analysis Overview\n",
    "\n",
    "## Source Information\n",
    "- **Platform**: Discord\n",
    "- **Server**: {formatted_data['metadata']['server']}\n",
    "- **Channel**: {formatted_data['metadata']['channel']}\n",
    "- **Analysis Date**: {formatted_data['metadata']['extraction_date']}\n",
    "- **Time Period**: {formatted_data['metadata']['date_range']['start']} to {formatted_data['metadata']['date_range']['end']}\n",
    "\n",
    "## Summary Statistics\n",
    "- **Total Messages**: {formatted_data['metadata']['total_messages']:,}\n",
    "- **Unique Participants**: {formatted_data['metadata']['unique_users']}\n",
    "- **Total Words**: {formatted_data['analytics']['content_analysis']['total_words']:,}\n",
    "- **Average Words per Message**: {formatted_data['analytics']['content_analysis']['avg_words_per_message']:.1f}\n",
    "- **Messages with Reactions**: {formatted_data['analytics']['content_analysis']['messages_with_reactions']}\n",
    "- **Edited Messages**: {formatted_data['analytics']['content_analysis']['edited_messages']}\n",
    "- **Messages with Mentions**: {formatted_data['analytics']['content_analysis']['messages_with_mentions']}\n",
    "\n",
    "## Key Insights\n",
    "This dataset represents {(pd.to_datetime(formatted_data['metadata']['date_range']['end']) - pd.to_datetime(formatted_data['metadata']['date_range']['start'])).days} days of community conversation in the STBL Discord channel, capturing authentic user interactions, discussions about cryptocurrency/tokens, and community dynamics.\n",
    "\"\"\"\n",
    "    \n",
    "    documents['overview'] = overview\n",
    "    \n",
    "    # 2. User Activity Analysis\n",
    "    user_stats = formatted_data['analytics']['user_stats']\n",
    "    top_users = sorted(user_stats.items(), key=lambda x: x[1]['message_count'], reverse=True)[:10]\n",
    "    \n",
    "    user_analysis = f\"\"\"# User Activity Analysis - STBL Discord Community\n",
    "\n",
    "## Most Active Participants\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    for i, (username, stats) in enumerate(top_users, 1):\n",
    "        user_analysis += f\"\"\"### {i}. {username}\n",
    "- **Messages**: {stats['message_count']}\n",
    "- **Total Words**: {stats['word_count']:,}\n",
    "- **Average Words/Message**: {stats['word_count']/stats['message_count']:.1f}\n",
    "- **Messages with Reactions**: {stats['has_reactions']}\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    user_analysis += f\"\"\"\n",
    "## Community Engagement Patterns\n",
    "- **Total Active Users**: {len(user_stats)}\n",
    "- **Most Prolific User**: {top_users[0][0]} ({top_users[0][1]['message_count']} messages)\n",
    "- **Community Distribution**: Shows {'high' if len(top_users) >= 5 else 'moderate'} engagement with multiple active participants\n",
    "\"\"\"\n",
    "    \n",
    "    documents['user_activity'] = user_analysis\n",
    "    \n",
    "    # 3. Temporal Analysis\n",
    "    daily_stats = formatted_data['analytics']['daily_activity']\n",
    "    hourly_stats = formatted_data['analytics']['hourly_pattern']\n",
    "    \n",
    "    temporal_analysis = f\"\"\"# Temporal Activity Patterns - STBL Discord\n",
    "\n",
    "## Daily Activity Summary\n",
    "Total active days: {len(daily_stats)}\n",
    "\n",
    "### Most Active Days\n",
    "\"\"\"\n",
    "    \n",
    "    top_days = sorted(daily_stats.items(), key=lambda x: x[1], reverse=True)[:7]\n",
    "    for date, count in top_days:\n",
    "        temporal_analysis += f\"- **{date}**: {count} messages\\n\"\n",
    "    \n",
    "    temporal_analysis += f\"\"\"\n",
    "## Hourly Activity Patterns\n",
    "Peak activity hours (24-hour format):\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    top_hours = sorted(hourly_stats.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    for hour, count in top_hours:\n",
    "        time_str = f\"{hour:02d}:00\"\n",
    "        temporal_analysis += f\"- **{time_str}**: {count} messages\\n\"\n",
    "    \n",
    "    documents['temporal_analysis'] = temporal_analysis\n",
    "    \n",
    "    # 4. Complete Message Archive\n",
    "    messages_archive = f\"\"\"# Complete Message Archive - STBL Discord Community\n",
    "\n",
    "## Chronological Conversation Log\n",
    "\n",
    "This section contains the complete chronological record of all messages extracted from the STBL Discord general chat channel.\n",
    "\n",
    "---\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # Sort messages chronologically\n",
    "    sorted_messages = processed_df.sort_values('timestamp_dt')\n",
    "    \n",
    "    current_date = None\n",
    "    for _, msg in sorted_messages.iterrows():\n",
    "        msg_date = msg['timestamp_dt'].date()\n",
    "        \n",
    "        # Add date header for new days\n",
    "        if current_date != msg_date:\n",
    "            current_date = msg_date\n",
    "            messages_archive += f\"\\n## {msg_date.strftime('%A, %B %d, %Y')}\\n\\n\"\n",
    "        \n",
    "        # Format timestamp\n",
    "        time_str = msg['timestamp_dt'].strftime('%H:%M')\n",
    "        \n",
    "        # Add message\n",
    "        username = msg['username'] or 'Unknown User'\n",
    "        content = msg['content_clean']\n",
    "        \n",
    "        # Add edit indicator\n",
    "        edit_indicator = \" *(edited)*\" if msg['is_edited'] else \"\"\n",
    "        \n",
    "        # Add reaction indicator  \n",
    "        reaction_indicator = \" ğŸ”¥\" if msg['has_reactions'] else \"\"\n",
    "        \n",
    "        messages_archive += f\"**{time_str} - {username}**{edit_indicator}{reaction_indicator}\\n\"\n",
    "        messages_archive += f\"{content}\\n\\n\"\n",
    "    \n",
    "    documents['complete_archive'] = messages_archive\n",
    "    \n",
    "    # 5. Content Analysis\n",
    "    all_content = ' '.join(processed_df['content_clean'].dropna().astype(str))\n",
    "    \n",
    "    # Extract potential topics/keywords\n",
    "    words = re.findall(r'\\b[a-zA-Z]{4,}\\b', all_content.lower())\n",
    "    word_freq = Counter(words)\n",
    "    \n",
    "    # Filter out common words\n",
    "    stopwords = {'that', 'this', 'with', 'they', 'have', 'will', 'from', 'been', 'were', 'what', 'when', 'where', 'just', 'like', 'more', 'some', 'said', 'only', 'very', 'well', 'also', 'much', 'good', 'make', 'time', 'know', 'think'}\n",
    "    relevant_words = [(word, count) for word, count in word_freq.most_common(20) if word not in stopwords]\n",
    "    \n",
    "    content_analysis = f\"\"\"# Content & Topic Analysis - STBL Discord\n",
    "\n",
    "## Key Discussion Topics\n",
    "\n",
    "Based on frequency analysis of message content:\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    for word, count in relevant_words[:10]:\n",
    "        content_analysis += f\"- **{word.title()}**: mentioned {count} times\\n\"\n",
    "    \n",
    "    content_analysis += f\"\"\"\n",
    "\n",
    "## Communication Characteristics\n",
    "- **Average Message Length**: {formatted_data['analytics']['content_analysis']['avg_words_per_message']:.1f} words\n",
    "- **Total Vocabulary**: {len(set(words)):,} unique words\n",
    "- **Conversation Style**: {\"Interactive\" if formatted_data['analytics']['content_analysis']['messages_with_mentions'] > 10 else \"Broadcast\"}\n",
    "- **Engagement Level**: {\"High\" if formatted_data['analytics']['content_analysis']['messages_with_reactions'] > 20 else \"Moderate\"}\n",
    "\n",
    "## Notable Patterns\n",
    "- {formatted_data['analytics']['content_analysis']['edited_messages']} messages were edited, showing {('high' if formatted_data['analytics']['content_analysis']['edited_messages'] / formatted_data['metadata']['total_messages'] > 0.1 else 'low')} revision rate\n",
    "- Community shows {'active' if len(formatted_data['analytics']['user_stats']) > 5 else 'modest'} participation with {formatted_data['metadata']['unique_users']} unique contributors\n",
    "\"\"\"\n",
    "    \n",
    "    documents['content_analysis'] = content_analysis\n",
    "    \n",
    "    print(f\"ğŸ“„ Created {len(documents)} structured documents:\")\n",
    "    for doc_name, content in documents.items():\n",
    "        word_count = len(content.split())\n",
    "        print(f\"  - {doc_name}: {word_count:,} words\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Create structured documents\n",
    "if formatted_data and not processed_df.empty:\n",
    "    structured_documents = create_structured_documents(formatted_data, processed_df)\n",
    "else:\n",
    "    print(\"âš ï¸  Skipping document creation - no formatted data available\")\n",
    "    structured_documents = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe8c1fc",
   "metadata": {},
   "source": [
    "## 5. Export to Text Files\n",
    "Write formatted documents to individual text files or consolidated formats that NotebookLM can process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db29f6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_files(structured_documents, formatted_data):\n",
    "    \"\"\"Export formatted documents to text files for NotebookLM\"\"\"\n",
    "    \n",
    "    if not structured_documents:\n",
    "        print(\"âŒ No documents to export\")\n",
    "        return []\n",
    "    \n",
    "    exported_files = []\n",
    "    \n",
    "    # Create timestamp for file naming\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Export individual documents\n",
    "    for doc_name, content in structured_documents.items():\n",
    "        filename = f\"discord_stbl_{doc_name}_{timestamp}.txt\"\n",
    "        filepath = EXPORT_DIR / filename\n",
    "        \n",
    "        try:\n",
    "            with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                f.write(content)\n",
    "            \n",
    "            exported_files.append({\n",
    "                'name': doc_name,\n",
    "                'filename': filename,\n",
    "                'filepath': filepath,\n",
    "                'size_kb': filepath.stat().st_size / 1024,\n",
    "                'word_count': len(content.split())\n",
    "            })\n",
    "            \n",
    "            print(f\"âœ… Exported {doc_name}: {filepath.name} ({len(content.split()):,} words)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to export {doc_name}: {e}\")\n",
    "    \n",
    "    # Create consolidated document\n",
    "    consolidated_content = f\"\"\"# Complete STBL Discord Community Analysis\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "This document contains comprehensive analysis of Discord messages from the STBL community general chat channel.\n",
    "\n",
    "{'='*80}\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    for doc_name, content in structured_documents.items():\n",
    "        consolidated_content += f\"\\n{content}\\n\\n{'='*80}\\n\"\n",
    "    \n",
    "    # Export consolidated document\n",
    "    consolidated_filename = f\"discord_stbl_complete_analysis_{timestamp}.txt\"\n",
    "    consolidated_filepath = EXPORT_DIR / consolidated_filename\n",
    "    \n",
    "    try:\n",
    "        with open(consolidated_filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(consolidated_content)\n",
    "        \n",
    "        consolidated_info = {\n",
    "            'name': 'consolidated_analysis',\n",
    "            'filename': consolidated_filename,\n",
    "            'filepath': consolidated_filepath,\n",
    "            'size_kb': consolidated_filepath.stat().st_size / 1024,\n",
    "            'word_count': len(consolidated_content.split())\n",
    "        }\n",
    "        \n",
    "        exported_files.append(consolidated_info)\n",
    "        print(f\"âœ… Exported consolidated analysis: {consolidated_filename} ({len(consolidated_content.split()):,} words)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to export consolidated document: {e}\")\n",
    "    \n",
    "    # Export raw data as JSON (for advanced analysis)\n",
    "    if formatted_data:\n",
    "        json_filename = f\"discord_stbl_raw_data_{timestamp}.json\"\n",
    "        json_filepath = EXPORT_DIR / json_filename\n",
    "        \n",
    "        try:\n",
    "            with open(json_filepath, 'w', encoding='utf-8') as f:\n",
    "                json.dump(formatted_data, f, indent=2, default=str)\n",
    "            \n",
    "            json_info = {\n",
    "                'name': 'raw_data_json',\n",
    "                'filename': json_filename,\n",
    "                'filepath': json_filepath,\n",
    "                'size_kb': json_filepath.stat().st_size / 1024,\n",
    "                'word_count': 'N/A (JSON format)'\n",
    "            }\n",
    "            \n",
    "            exported_files.append(json_info)\n",
    "            print(f\"âœ… Exported raw data: {json_filename}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to export raw JSON: {e}\")\n",
    "    \n",
    "    return exported_files\n",
    "\n",
    "# Export all files\n",
    "if structured_documents:\n",
    "    exported_files = export_to_files(structured_documents, formatted_data)\n",
    "    \n",
    "    # Summary of exports\n",
    "    print(f\"\\nğŸ“¤ EXPORT SUMMARY\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Total files exported: {len(exported_files)}\")\n",
    "    print(f\"Export directory: {EXPORT_DIR}\")\n",
    "    \n",
    "    total_size_mb = sum(f['size_kb'] for f in exported_files) / 1024\n",
    "    print(f\"Total size: {total_size_mb:.2f} MB\")\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ File Details:\")\n",
    "    for file_info in exported_files:\n",
    "        print(f\"  ğŸ“„ {file_info['filename']}\")\n",
    "        print(f\"     Size: {file_info['size_kb']:.1f} KB | Words: {file_info['word_count']}\")\n",
    "        \n",
    "else:\n",
    "    print(\"âš ï¸  Skipping file export - no documents available\")\n",
    "    exported_files = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fdab82",
   "metadata": {},
   "source": [
    "## 6. Generate Metadata\n",
    "Create accompanying metadata files with document information, tags, and source references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a51f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_metadata(exported_files, formatted_data):\n",
    "    \"\"\"Generate comprehensive metadata for NotebookLM import\"\"\"\n",
    "    \n",
    "    if not exported_files or not formatted_data:\n",
    "        print(\"âŒ No data available for metadata generation\")\n",
    "        return None\n",
    "    \n",
    "    # Create comprehensive metadata\n",
    "    metadata = {\n",
    "        \"dataset_info\": {\n",
    "            \"title\": \"STBL Discord Community Chat Analysis\",\n",
    "            \"description\": \"Comprehensive analysis of Discord messages from STBL general chat channel, including user activity, temporal patterns, and content analysis\",\n",
    "            \"source_platform\": \"Discord\",\n",
    "            \"server_name\": formatted_data['metadata']['server'],\n",
    "            \"channel_name\": formatted_data['metadata']['channel'],\n",
    "            \"extraction_method\": \"Enhanced browser automation with Playwright\",\n",
    "            \"data_quality\": \"High-fidelity extraction with 90%+ success rate\"\n",
    "        },\n",
    "        \n",
    "        \"temporal_coverage\": {\n",
    "            \"start_date\": formatted_data['metadata']['date_range']['start'],\n",
    "            \"end_date\": formatted_data['metadata']['date_range']['end'],\n",
    "            \"total_days\": (pd.to_datetime(formatted_data['metadata']['date_range']['end']) - \n",
    "                          pd.to_datetime(formatted_data['metadata']['date_range']['start'])).days,\n",
    "            \"extraction_date\": formatted_data['metadata']['extraction_date']\n",
    "        },\n",
    "        \n",
    "        \"content_statistics\": {\n",
    "            \"total_messages\": formatted_data['metadata']['total_messages'],\n",
    "            \"unique_users\": formatted_data['metadata']['unique_users'],\n",
    "            \"total_words\": formatted_data['analytics']['content_analysis']['total_words'],\n",
    "            \"average_words_per_message\": formatted_data['analytics']['content_analysis']['avg_words_per_message'],\n",
    "            \"messages_with_reactions\": formatted_data['analytics']['content_analysis']['messages_with_reactions'],\n",
    "            \"edited_messages\": formatted_data['analytics']['content_analysis']['edited_messages'],\n",
    "            \"messages_with_mentions\": formatted_data['analytics']['content_analysis']['messages_with_mentions']\n",
    "        },\n",
    "        \n",
    "        \"files_exported\": [\n",
    "            {\n",
    "                \"filename\": f['filename'],\n",
    "                \"document_type\": f['name'],\n",
    "                \"size_kb\": f['size_kb'],\n",
    "                \"word_count\": f['word_count'],\n",
    "                \"description\": get_file_description(f['name'])\n",
    "            }\n",
    "            for f in exported_files\n",
    "        ],\n",
    "        \n",
    "        \"analysis_categories\": [\n",
    "            \"Community Engagement Analysis\",\n",
    "            \"User Activity Patterns\", \n",
    "            \"Temporal Behavior Analysis\",\n",
    "            \"Content & Topic Mining\",\n",
    "            \"Complete Message Archive\",\n",
    "            \"Cryptocurrency Discussion Patterns\"\n",
    "        ],\n",
    "        \n",
    "        \"recommended_analysis\": {\n",
    "            \"primary_focus\": [\n",
    "                \"Community dynamics and user engagement patterns\",\n",
    "                \"Discussion topics and cryptocurrency sentiment\",\n",
    "                \"Temporal activity patterns and peak engagement times\",\n",
    "                \"User interaction networks and influence patterns\"\n",
    "            ],\n",
    "            \"key_questions\": [\n",
    "                \"What are the main discussion topics in the STBL community?\",\n",
    "                \"Who are the most influential community members?\",\n",
    "                \"When is the community most active?\",\n",
    "                \"How does the community respond to market events?\",\n",
    "                \"What is the sentiment around STBL token discussions?\"\n",
    "            ]\n",
    "        },\n",
    "        \n",
    "        \"technical_details\": {\n",
    "            \"extraction_algorithm\": \"Enhanced scrolling with 4 methods and 20-failure tolerance\",\n",
    "            \"data_completeness\": \"Complete channel history captured\",\n",
    "            \"timestamp_precision\": \"UTC timezone with full datetime precision\",\n",
    "            \"content_preservation\": \"Full message content including edits, reactions, and mentions\",\n",
    "            \"schema_version\": \"SignalSifter Discord v1.0\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "def get_file_description(doc_type):\n",
    "    \"\"\"Get description for each document type\"\"\"\n",
    "    descriptions = {\n",
    "        'overview': 'High-level summary and statistics of the Discord community data',\n",
    "        'user_activity': 'Detailed analysis of user participation and engagement patterns',\n",
    "        'temporal_analysis': 'Time-based activity patterns including daily and hourly trends',\n",
    "        'complete_archive': 'Chronological archive of all extracted Discord messages',\n",
    "        'content_analysis': 'Topic analysis, keywords, and communication characteristics',\n",
    "        'consolidated_analysis': 'Complete analysis combining all sections for comprehensive review',\n",
    "        'raw_data_json': 'Structured JSON data for programmatic analysis and advanced queries'\n",
    "    }\n",
    "    return descriptions.get(doc_type, 'Supporting analysis document')\n",
    "\n",
    "# Generate metadata\n",
    "if exported_files and formatted_data:\n",
    "    metadata = generate_metadata(exported_files, formatted_data)\n",
    "    \n",
    "    if metadata:\n",
    "        # Export metadata file\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        metadata_filename = f\"discord_stbl_metadata_{timestamp}.json\"\n",
    "        metadata_filepath = EXPORT_DIR / metadata_filename\n",
    "        \n",
    "        try:\n",
    "            with open(metadata_filepath, 'w', encoding='utf-8') as f:\n",
    "                json.dump(metadata, f, indent=2, default=str)\n",
    "            \n",
    "            print(f\"âœ… Generated metadata: {metadata_filename}\")\n",
    "            \n",
    "            # Create human-readable metadata\n",
    "            readme_content = f\"\"\"# STBL Discord Community Analysis - Dataset Documentation\n",
    "\n",
    "## Overview\n",
    "{metadata['dataset_info']['description']}\n",
    "\n",
    "## Data Source\n",
    "- **Platform**: {metadata['dataset_info']['source_platform']}\n",
    "- **Server**: {metadata['dataset_info']['server_name']}\n",
    "- **Channel**: {metadata['dataset_info']['channel_name']}\n",
    "- **Extraction Date**: {metadata['temporal_coverage']['extraction_date'][:10]}\n",
    "\n",
    "## Dataset Statistics\n",
    "- **Messages**: {metadata['content_statistics']['total_messages']:,}\n",
    "- **Users**: {metadata['content_statistics']['unique_users']}\n",
    "- **Time Period**: {metadata['temporal_coverage']['start_date'][:10]} to {metadata['temporal_coverage']['end_date'][:10]}\n",
    "- **Total Days**: {metadata['temporal_coverage']['total_days']}\n",
    "- **Total Words**: {metadata['content_statistics']['total_words']:,}\n",
    "\n",
    "## Files Included\n",
    "\n",
    "\"\"\"\n",
    "            \n",
    "            for file_info in metadata['files_exported']:\n",
    "                readme_content += f\"\"\"### {file_info['filename']}\n",
    "- **Type**: {file_info['document_type'].replace('_', ' ').title()}\n",
    "- **Size**: {file_info['size_kb']:.1f} KB\n",
    "- **Content**: {file_info['description']}\n",
    "\n",
    "\"\"\"\n",
    "            \n",
    "            readme_content += f\"\"\"## Recommended Analysis Focus\n",
    "{chr(10).join(f\"- {focus}\" for focus in metadata['recommended_analysis']['primary_focus'])}\n",
    "\n",
    "## Key Research Questions\n",
    "{chr(10).join(f\"- {question}\" for question in metadata['recommended_analysis']['key_questions'])}\n",
    "\n",
    "## Usage in NotebookLM\n",
    "1. Import the consolidated analysis file for comprehensive overview\n",
    "2. Use individual files for focused analysis on specific aspects\n",
    "3. Reference the complete message archive for detailed context\n",
    "4. Use the JSON file for custom programmatic analysis\n",
    "\n",
    "## Data Quality Notes\n",
    "- **Extraction Method**: {metadata['technical_details']['extraction_algorithm']}\n",
    "- **Completeness**: {metadata['technical_details']['data_completeness']}\n",
    "- **Content Preservation**: {metadata['technical_details']['content_preservation']}\n",
    "\n",
    "---\n",
    "Generated by SignalSifter Discord Analysis System\n",
    "\"\"\"\n",
    "            \n",
    "            readme_filename = f\"README_discord_stbl_{timestamp}.md\"\n",
    "            readme_filepath = EXPORT_DIR / readme_filename\n",
    "            \n",
    "            with open(readme_filepath, 'w', encoding='utf-8') as f:\n",
    "                f.write(readme_content)\n",
    "            \n",
    "            print(f\"âœ… Generated README: {readme_filename}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to generate metadata: {e}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸  Skipping metadata generation - insufficient data\")\n",
    "    metadata = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9907fd",
   "metadata": {},
   "source": [
    "## 7. Validate Export Format\n",
    "Verify the exported files meet NotebookLM requirements and test sample imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a2e6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_export_format(exported_files):\n",
    "    \"\"\"Validate exported files for NotebookLM compatibility\"\"\"\n",
    "    \n",
    "    validation_results = []\n",
    "    \n",
    "    if not exported_files:\n",
    "        print(\"âŒ No files to validate\")\n",
    "        return validation_results\n",
    "    \n",
    "    print(\"ğŸ” VALIDATION REPORT\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for file_info in exported_files:\n",
    "        filepath = file_info['filepath']\n",
    "        \n",
    "        # Check file existence\n",
    "        if not filepath.exists():\n",
    "            validation_results.append({\n",
    "                'file': file_info['filename'],\n",
    "                'status': 'FAILED',\n",
    "                'issue': 'File not found',\n",
    "                'recommendation': 'Re-run export process'\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Check file size (NotebookLM has limits)\n",
    "        size_mb = file_info['size_kb'] / 1024\n",
    "        size_status = 'PASS'\n",
    "        size_issue = None\n",
    "        \n",
    "        if size_mb > 50:  # NotebookLM typical limit\n",
    "            size_status = 'WARNING'\n",
    "            size_issue = f'Large file ({size_mb:.1f}MB) may need splitting'\n",
    "        elif size_mb < 0.1:\n",
    "            size_status = 'WARNING'  \n",
    "            size_issue = f'Very small file ({size_mb:.3f}MB) may lack content'\n",
    "        \n",
    "        # Check content format for text files\n",
    "        if filepath.suffix == '.txt':\n",
    "            try:\n",
    "                with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read(1000)  # Read first 1KB\n",
    "                \n",
    "                # Check for proper text encoding\n",
    "                content_status = 'PASS'\n",
    "                content_issue = None\n",
    "                \n",
    "                if len(content.strip()) == 0:\n",
    "                    content_status = 'FAILED'\n",
    "                    content_issue = 'Empty content'\n",
    "                elif not content.isprintable() and not any(c in content for c in ['\\n', '\\t', '\\r']):\n",
    "                    content_status = 'WARNING'\n",
    "                    content_issue = 'May contain non-printable characters'\n",
    "                \n",
    "                # Check for structured format\n",
    "                has_headers = bool(re.search(r'^#+ ', content, re.MULTILINE))\n",
    "                structure_status = 'PASS' if has_headers else 'WARNING'\n",
    "                structure_issue = None if has_headers else 'No markdown headers detected'\n",
    "                \n",
    "            except Exception as e:\n",
    "                content_status = 'FAILED'\n",
    "                content_issue = f'Content read error: {e}'\n",
    "                structure_status = 'FAILED'\n",
    "                structure_issue = 'Cannot analyze structure'\n",
    "        \n",
    "        elif filepath.suffix == '.json':\n",
    "            try:\n",
    "                with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                    json_data = json.load(f)\n",
    "                \n",
    "                content_status = 'PASS'\n",
    "                content_issue = None\n",
    "                structure_status = 'PASS'\n",
    "                structure_issue = None\n",
    "                \n",
    "            except Exception as e:\n",
    "                content_status = 'FAILED'\n",
    "                content_issue = f'Invalid JSON: {e}'\n",
    "                structure_status = 'FAILED'\n",
    "                structure_issue = 'JSON parse error'\n",
    "        \n",
    "        else:\n",
    "            content_status = 'UNKNOWN'\n",
    "            content_issue = 'Unknown file format'\n",
    "            structure_status = 'UNKNOWN'\n",
    "            structure_issue = 'Cannot validate structure'\n",
    "        \n",
    "        # Overall status\n",
    "        if content_status == 'FAILED' or structure_status == 'FAILED':\n",
    "            overall_status = 'FAILED'\n",
    "        elif content_status == 'WARNING' or structure_status == 'WARNING' or size_status == 'WARNING':\n",
    "            overall_status = 'WARNING'\n",
    "        else:\n",
    "            overall_status = 'PASS'\n",
    "        \n",
    "        # Compile validation result\n",
    "        result = {\n",
    "            'file': file_info['filename'],\n",
    "            'status': overall_status,\n",
    "            'size_mb': size_mb,\n",
    "            'size_status': size_status,\n",
    "            'size_issue': size_issue,\n",
    "            'content_status': content_status,\n",
    "            'content_issue': content_issue,\n",
    "            'structure_status': structure_status,\n",
    "            'structure_issue': structure_issue,\n",
    "            'notebooklm_ready': overall_status in ['PASS', 'WARNING']\n",
    "        }\n",
    "        \n",
    "        validation_results.append(result)\n",
    "        \n",
    "        # Print result\n",
    "        status_emoji = {'PASS': 'âœ…', 'WARNING': 'âš ï¸', 'FAILED': 'âŒ', 'UNKNOWN': 'â“'}\n",
    "        print(f\"{status_emoji[overall_status]} {file_info['filename']}\")\n",
    "        print(f\"   Size: {size_mb:.1f}MB {status_emoji[size_status]}\")\n",
    "        print(f\"   Content: {status_emoji[content_status]}\")\n",
    "        print(f\"   Structure: {status_emoji[structure_status]}\")\n",
    "        \n",
    "        if any([size_issue, content_issue, structure_issue]):\n",
    "            print(f\"   Issues: {', '.join(filter(None, [size_issue, content_issue, structure_issue]))}\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    # Summary\n",
    "    total_files = len(validation_results)\n",
    "    passed_files = len([r for r in validation_results if r['status'] == 'PASS'])\n",
    "    warning_files = len([r for r in validation_results if r['status'] == 'WARNING'])\n",
    "    failed_files = len([r for r in validation_results if r['status'] == 'FAILED'])\n",
    "    ready_files = len([r for r in validation_results if r['notebooklm_ready']])\n",
    "    \n",
    "    print(f\"ğŸ“Š VALIDATION SUMMARY\")\n",
    "    print(f\"   Total files: {total_files}\")\n",
    "    print(f\"   âœ… Passed: {passed_files}\")\n",
    "    print(f\"   âš ï¸  Warnings: {warning_files}\")\n",
    "    print(f\"   âŒ Failed: {failed_files}\")\n",
    "    print(f\"   ğŸ¯ NotebookLM Ready: {ready_files}/{total_files}\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(f\"\\nğŸ’¡ NOTEBOOKLM IMPORT RECOMMENDATIONS\")\n",
    "    \n",
    "    if ready_files == total_files:\n",
    "        print(\"ğŸ‰ All files are ready for NotebookLM import!\")\n",
    "        print(\"ğŸ“‹ Suggested import order:\")\n",
    "        print(\"   1. Start with the consolidated analysis for overview\")\n",
    "        print(\"   2. Import individual analysis documents for focused study\")\n",
    "        print(\"   3. Use the complete archive for detailed message context\")\n",
    "        print(\"   4. Reference metadata files for dataset understanding\")\n",
    "    else:\n",
    "        print(\"âš ï¸  Some files need attention before NotebookLM import\")\n",
    "        \n",
    "        for result in validation_results:\n",
    "            if not result['notebooklm_ready']:\n",
    "                print(f\"   ğŸ”§ {result['file']}: {result['content_issue'] or result['structure_issue']}\")\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "# Validate exported files\n",
    "if exported_files:\n",
    "    validation_results = validate_export_format(exported_files)\n",
    "    \n",
    "    print(f\"\\nğŸ¯ FINAL EXPORT STATUS\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    ready_count = sum(1 for r in validation_results if r['notebooklm_ready'])\n",
    "    total_count = len(validation_results)\n",
    "    \n",
    "    if ready_count == total_count and total_count > 0:\n",
    "        print(f\"âœ… SUCCESS: All {total_count} files ready for NotebookLM analysis!\")\n",
    "        print(f\"ğŸ“‚ Files location: {EXPORT_DIR}\")\n",
    "        print(f\"\\nğŸš€ Next steps:\")\n",
    "        print(f\"   1. Navigate to {EXPORT_DIR}\")\n",
    "        print(f\"   2. Upload files to NotebookLM\")\n",
    "        print(f\"   3. Start with the consolidated analysis document\")\n",
    "        print(f\"   4. Use the README file for guidance\")\n",
    "    else:\n",
    "        print(f\"âš ï¸  Status: {ready_count}/{total_count} files ready\")\n",
    "        print(f\"ğŸ“‚ Files location: {EXPORT_DIR}\")\n",
    "        print(f\"ğŸ”§ Review validation issues above\")\n",
    "        \n",
    "else:\n",
    "    print(\"âŒ No files were exported to validate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f8adca",
   "metadata": {},
   "source": [
    "## 8. Execute Complete Pipeline\n",
    "\n",
    "This section runs the entire export pipeline from data loading to validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e811fc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸš€ STARTING COMPLETE NOTEBOOKLM EXPORT PIPELINE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Load Discord data\n",
    "print(\"\\nğŸ“Š Step 1: Loading Discord data...\")\n",
    "try:\n",
    "    messages_df, users_df, channels_df = load_discord_data()\n",
    "    print(f\"âœ… Loaded {len(messages_df)} messages from {len(channels_df)} channels\")\n",
    "    print(f\"âœ… Found {len(users_df)} unique users\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Failed to load data: {e}\")\n",
    "    raise\n",
    "\n",
    "# Step 2: Format data for NotebookLM\n",
    "print(\"\\nğŸ“ Step 2: Formatting data for NotebookLM...\")\n",
    "try:\n",
    "    formatted_data = format_for_notebooklm(messages_df, users_df, channels_df)\n",
    "    print(f\"âœ… Generated {len(formatted_data)} formatted documents\")\n",
    "    \n",
    "    # Show what we generated\n",
    "    for doc_type, content in formatted_data.items():\n",
    "        word_count = len(content.split())\n",
    "        print(f\"   ğŸ“„ {doc_type}: {word_count:,} words\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Failed to format data: {e}\")\n",
    "    raise\n",
    "\n",
    "# Step 3: Create structured documents\n",
    "print(\"\\nğŸ“š Step 3: Creating structured documents...\")\n",
    "try:\n",
    "    structured_docs = create_structured_documents(formatted_data, messages_df, users_df)\n",
    "    print(f\"âœ… Created {len(structured_docs)} structured documents\")\n",
    "    \n",
    "    for doc_name, doc_content in structured_docs.items():\n",
    "        word_count = len(doc_content.split())\n",
    "        print(f\"   ğŸ“‘ {doc_name}: {word_count:,} words\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Failed to create structured documents: {e}\")\n",
    "    raise\n",
    "\n",
    "# Step 4: Export to files\n",
    "print(\"\\nğŸ’¾ Step 4: Exporting to files...\")\n",
    "try:\n",
    "    exported_files = export_to_files(structured_docs, messages_df, users_df, channels_df)\n",
    "    print(f\"âœ… Exported {len(exported_files)} files\")\n",
    "    \n",
    "    total_size_mb = sum(f['size_kb'] for f in exported_files) / 1024\n",
    "    print(f\"ğŸ“ Total export size: {total_size_mb:.1f}MB\")\n",
    "    \n",
    "    for file_info in exported_files:\n",
    "        print(f\"   ğŸ“ {file_info['filename']}: {file_info['size_kb']:.1f}KB\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Failed to export files: {e}\")\n",
    "    raise\n",
    "\n",
    "# Step 5: Generate metadata\n",
    "print(\"\\nğŸ·ï¸ Step 5: Generating metadata...\")\n",
    "try:\n",
    "    metadata = generate_metadata(messages_df, users_df, channels_df)\n",
    "    print(f\"âœ… Generated comprehensive metadata\")\n",
    "    \n",
    "    # Show key statistics\n",
    "    print(f\"   ğŸ“… Date range: {metadata['data_summary']['date_range']}\")\n",
    "    print(f\"   ğŸ’¬ Message count: {metadata['data_summary']['message_count']:,}\")\n",
    "    print(f\"   ğŸ‘¥ Active users: {metadata['data_summary']['active_users']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Failed to generate metadata: {e}\")\n",
    "    raise\n",
    "\n",
    "# Step 6: Validate export format\n",
    "print(\"\\nğŸ” Step 6: Validating export format...\")\n",
    "try:\n",
    "    validation_results = validate_export_format(exported_files)\n",
    "    \n",
    "    ready_count = sum(1 for r in validation_results if r['notebooklm_ready'])\n",
    "    total_count = len(validation_results)\n",
    "    \n",
    "    if ready_count == total_count:\n",
    "        print(f\"âœ… All {total_count} files passed validation!\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ {ready_count}/{total_count} files ready for NotebookLM\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Failed to validate export: {e}\")\n",
    "    raise\n",
    "\n",
    "print(f\"\\nğŸ‰ EXPORT PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"=\" * 60)\n",
    "print(f\"ğŸ“‚ Export location: {EXPORT_DIR}\")\n",
    "print(f\"ğŸ“‹ Files ready for NotebookLM analysis\")\n",
    "print(f\"\\nğŸ¯ Next steps:\")\n",
    "print(f\"   1. Navigate to the export directory\")\n",
    "print(f\"   2. Review the README file for import guidance\") \n",
    "print(f\"   3. Upload documents to NotebookLM\")\n",
    "print(f\"   4. Start analysis with the consolidated overview\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
